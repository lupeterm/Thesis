\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Evaluation}

In diesem kapitel werden zuvor ausgewählte algorithmen einheitlich verglichen und die resultierenden ergebnisse ausgewertet.

\section{Evaluation Protocol}
Das Ziel der Arbeit ist es, die Frage zu beantworten, ob präzise Ebenenfindung in Echtzeit möglich ist.
Das Problem bei der Auswahl des "besten" algos ist, dass jeder Algorithmus auf anderen Daten getestet wurde, was die
Vergleichbarkeit dezimiert.
Daher wird diese Evaluation auf einem öffentlich erhältlichen Datensatz ausgeführt, sowie einem "live" test unterzogen.
Der Datensatz besteht aus einer Anzahl indoor Aufnahmen mit dazugehöriger Ground Truth, beides in Punktwolken-Form.

grundlegend: \\
- Problem: niemand nimmt den gleichen Datensatz \\
- daher alle aufm selben testen für einheitlichen vergleich \\
- quantitativ: datensatz+GT \\
- qualitativ: "live" test in der FIN, überlagern der ebenen mit punktwolke \\
- diese metriken sind wichtig: \\


\subsection{Metrics}
grundlegend: \\
- Laufzeit in relation zu len(PC) und \#planes \\
- Präzision, Recall: "best match" berechnen. wie? i guess über MSE, wenn ich eh die cornerpoints habe. Dafür muss ich jedoch eine maximale entfernung angeben, bis eine ebene kein match hat\\
- f1 i guess, tut genauso weh, es raus zu lassen, wie eine weitere spalte in die tabelle zu klatschen. man kann es ja eh aus P und R berechnen \\
- MSE: durchschnittlicher abstand der cornerpoints \\

Für die Bewertung eines Algos werden zum einen verschiedene Laufzeiten betrachtet. Wie lange dauert das finden von Ebenen
in relation zur Größe der Punktwolke, wie lange dauert es pro Ebene.

Dazu werden wir die Qualität der Algorithmen bewerten. Dafür berechnen wir die \textit{precision}, \textit{recall}, den
\textit{f1-score} sowie eine Durchschnittliche Abweichung der Ebene von der gegebenen Ground truth über
den \textit{Mean-Squared-Error}(MSE).

\textcolor{red}{Erklärung der Metriken im Background nehme ich an?}
Um eine gefundene Ebene einer Ebene der Ground truth zuzuordnen wird wird einfach die ebene mit der geringsten Abweichung
(geringster MSE) ausgewählt. Ist der MSE aller gefundenen Ebenen zu groß, wird die bestimmte Ebene als "nicht gefunden" gewertet.

\textcolor{red}{Was, wenn zu viele gefunden wurden? 1. wurden N ebenen doppelt gefunden oder Falsche ?}
Daraus ergeben sich die Werte für \textit{precision}, \textit{recall} und \textit{f1-score}.

Im anschluss betrachten wir das Maß an Korrektheit der Anordnung innerhalb der Scene. Im grunde berechnet sich dieses Maß
erneut durch den MSE. \textcolor{red}{quasi 3d variante zu IoU}


\subsection{Dataset}
Wir benutzen den \textit{Stanford Large-Scale Indoor Spaces 3D Dataset} (S3DIS)\cite[]{armeni_cvpr16}.
Der Datensatz beinhalt viele verschiedene Räume aus verschiedenen Gebäuden. Diese Indoor Umgebungen werden als Punktwolken
dargestellt.
Dazu werden Ground truths in form von annotierten punktwolken bereitgestellt, welche in insgesamt 12 Kategorien fallen.
Da für diese arbeit allein die Ebenen interessant sind, werden wir aus diesen GTs manuell planare Strukturen extrahieren
und durch ein convexes Polygon(\textcolor{red}{meistens 4 Ecken?}) beschreiben.

\subsection{Real-Life Test}
Bei dem Live test wird im Gebäude der FIN ein Datensatz aufgenommen. Der Scan wird \textcolor{red}{\$STUFF} beinhalten.
Zu diesem Scan gibt es keine Ground Truth, daher wird neben der Laufzeitanalyse lediglich eine qualitative Analyse der
Präzision vorgenommen.
Dafür überlagern wir die Punktwolke mit den gefundenen Ebenen.

\section{Results}
\subsection{Results Dataset}

Alle \textcolor{red}{N} Sequenzen des Datensatzs wurde \textcolor{red}{X} mal von jedem Algorithmus berechnet.

Hier sind die Ergebnisse:

\subsection{Results Real-Life Test}

Der FIN Datensatz wurde auch \textcolor{red}{X} mal von jedem Datensatz berechnet.

Hier sind die Ergebnisse:


\end{document}